{
  "kubernetes-cluster.md": "This is the story of when I was a young, innocent programmer. Fancy buzzwords like *Kubernetes*, *Docker*, and *Firebase* sounded magical to me — so of course, I had to use them.\n\nIn the summer of 2024, I was tasked with scraping data of about 6,600 doctors from **Practo.com** (an online healthcare platform where doctors and clinics are listed). One of my professors needed this data for a survey. The initial problem statement was simple:\n\n> Scrape doctor data from 6 cities across 6 specializations. (That's 36 queries total.)\n\n## Initial Approach\n\nThis looked like a straightforward task. Just reverse-engineer Practo's APIs, loop through those 36 queries, save the results as JSON, and merge them later. I had done API reverse-engineering before, so this didn't feel like a challenge.\n\nSo I wrote the code.\n\n![Code screenshot](./kubernetes/ray-so-export.png)\n*(Yes, I used `let` instead of `var` for `response` — I was young.)*\n\nI ran it. Within 2 minutes, Practo rate-limited me.\n\n## How do I bypass this rate limit?\n\nMy \"lazy\" solution: rerun the code whenever it crashed. Steve Jobs once said he'd only hire lazy people because they find shortcuts. I told myself this was one of those moments.\n\nIt *kinda* worked. But scraping all 6,600 doctors took two full days. Each API call only returned 10 doctors at a time — paginated. Those two days were hell. I had to babysit my laptop to make sure the program didn't stop. My poor laptop worked nonstop for 48 hours.\n\nI gave the data to my professor. A few weeks later, she came back:\n\n> \"Now scrape reviews for each doctor.\"\n\nAlarm bells. That meant 6,600 API calls — ten times more than before. By proportion, this would take **20 days**. For 20 days, my laptop couldn't shut down, couldn't unplug, couldn't rest. That wasn't sustainable.\n\nI asked my college CS department if I could SSH into one of their lab computers. They refused giving me one of their windows 10 computers. Luckily, my professor managed to arrange 5 lab PCs for me — but I had to physically go there to check progress. Pain.\n\n ## Solution \n\n I stumbled upon [this video by Harkirat Singh](https://www.youtube.com/watch?v=v06AYk-MnQ). He had a similar issue with scraping and rate limits. His solution? **Master–Worker architecture**: workers (any machine) take jobs from a master, possibly for a reward.\n\nI thought: *Perfect! I'll get workers from AWS EKS. Master can be a Firebase serverless function. Data in Firestore.*\n\nHere's what I built:\n\n![Architecture diagram](./kubernetes/Screenshot%20from%202025-10-04%2022-44-41.png)\n\nThe DB stored `doctorId` and `isReviewScraped`. Workers asked the master for tasks, and the master assigned pending ones.\n\n**Optimization:** Instead of sending one task per request, I batched 30 tasks to reduce latency.\n\nI separated master and worker logic, dockerized the worker, pushed it to Docker Hub, deployed the master on Firebase (with `minInstances = 1` to avoid cold starts). Then I wrote `deployment.yaml`, spun up an **EKS cluster with 16 t2 instances**, and hit \"Deploy.\"\n\nTo my surprise, all data was scraped in ~4 hours. From 20 days → 4 hours. I felt victorious.\n\nNext day, AWS sent me a ₹1700 (~$20) bill. My professor happily paid. I thought this was costly.\n\nSo why is this post titled the way it is?\n\n## The Plot Twist\n\nWe had to repeat scraping every week for different cities. One day, while reading logs, I noticed something weird:\n\nMultiple workers were scraping the **same doctorId**. Then I realized **all workers were scraping all doctorIds**.\n\nThat meant:\n\n* A **single EC2 instance** could've done the entire job in 4 hours.\n* My laptop took 2 days, but EC2 only 4 hours. Why?\n* If rate limits were IP-based, why did EC2 scale so much better?\n\nSo many questions.\n\n## The Answer\n\nTurns out Firestore was the culprit. It suffered from **race conditions**.\n\nNetwork variance between identical EC2 instances is minimal. So when multiple workers requested tasks almost simultaneously (`Δt < 250ms` by my guess), Firestore lagged in propagating updates (~1s delay). Result: the same tasks were handed out multiple times.\n\nIn short: Firestore isn't built for this type of locking.\n\n## The (Real) Solution\n\n* Firebase has no built-in locking.\n* Serverless functions are stateless, so semaphores don't help.\n* Trying to implement semaphores in Firestore just led to more race conditions.\n\nOnly real solution: **SQL databases**. They have built-in locks. No nonsense.\n\n## But Why Was One EC2 Instance Faster Than My Laptop?\n\nMy laptop's bandwidth was shared between scraping, YouTube, browsing, maybe some gaming. All those micro-delays added up. The EC2 instance had a dedicated, stable network — hence much faster scraping.\n\n## Lessons learned\n\nI learned that before scaling up your architecture, you should first make sure there are no inefficiencies in the current setup. In my case, I could have achieved the same ~4-hour performance by simply renting a single EC2 instance — at a much lower cost and without the hassle of setting up an entire EKS cluster."
}